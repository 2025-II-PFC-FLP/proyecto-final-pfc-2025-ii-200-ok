#  **# Informe de Paralelizaci√≥n ‚Äî Programaci√≥n de Riego**


# 1. **Introducci√≥n**

El presente informe documenta el proceso de **paralelizaci√≥n del sistema de c√°lculo de programaciones de riego** presentado en el proyecto. El sistema original contaba √∫nicamente con implementaciones **secuenciales**, mientras que el objetivo fue implementar alternativas **paralelas**, analizar su comportamiento y compararlas mediante _benchmarking_ usando **ScalaMeter**.

La paralelizaci√≥n se aplic√≥ a cuatro componentes principales:

1.  **C√°lculo del Tiempo de Inicio de Riego (tIR)**
    
2.  **C√°lculo del costo total (Costos de riego + movilidad)**
    
3.  **Generaci√≥n de permutaciones (programaciones de riego)**
    
4.  **B√∫squeda de la programaci√≥n √≥ptima**
    

Finalmente, se analizaron los resultados experimentales y se calcul√≥ el **speedup** para cada m√≥dulo y tama√±o de entrada.

----------

# 2. **Paralelizaci√≥n realizada**

A continuaci√≥n, se describe la estrategia utilizada en cada funci√≥n paralela del archivo `Paralelo.scala`.

----------

## 2.1 `tIRPar`: C√°lculo paralelo del tiempo de inicio de riego

### üîß Estrategia aplicada

-   Se usa un **arreglo buffer compartido** de tama√±o `n`.
    
-   Se calcula el vector de tiempos acumulados secuencialmente (su costo es insignificante).
    
-   Se paraleliza √∫nicamente la distribuci√≥n de los tiempos sobre los tablones usando:
    

```scala
pi.zip(acumulados.init).par.foreach(...)
```

###  Razonamiento

La operaci√≥n crucial (asignar el tiempo a cada tabl√≥n) es independiente, perfecta para map paralelo.

### ‚úî Complejidad

-   Secuencial: $O(n)$
    
-   Paralela: $O(n / p) + overhead$
    

----------

## 2.2 `costoRiegoFincaPar` y `costoMovilidadPar`

### Estrategia aplicada

-   Ambos usan **map paralelo**, ya que el costo de cada tabl√≥n y cada transici√≥n es independiente:
    

```scala
tiempos.indices.toVector.par.map(...)
pi.sliding(2).toVector.par.map(...)
```

### ‚úî Complejidad

-   Secuencial: $O(n)$
    
-   Paralela: $O(n/p) + overhead$
    

----------

## 2.3 `generarProgramacionesRiegoPar`

###  Estrategia aplicada

El m√≥dulo m√°s costoso del sistema es la generaci√≥n de permutaciones, cuyo costo es factorial:

-   Se paraleliza la rama superior del √°rbol de recursi√≥n:
    

```scala
v.par.flatMap { elem => ... }
```

Cada rama genera `aux(resto)`, que sigue siendo recursivo y costoso.

###  Razonamiento

Paralelizar en niveles muy profundos solo aumenta overhead.  
Por eso se paraleliza solo **la capa superior** (similar al patr√≥n "parallel-map" de permutaciones).

----------

## 2.4 `programacionRiegoOptimoPar`

###  Estrategia aplicada

Consiste en:

1.  Generar todas las programaciones (paralelo)
    
2.  Calcular sus costos (map paralelo)
    

```scala
todas.par.map(pi => (pi, costoTotalPar(f, pi, d)))
```

### ‚úî Complejidad

-   Secuencial: $O(n! √ó n)$
    
-   Paralela: $O((n! √ó n)/p) + overhead$
    

----------

# 3. **Benchmarking y Speedup**

Los datos fueron obtenidos con ScalaMeter evaluando tres tama√±os:  
**n = 6, 7 y 8 tablones.**

----------

##  **Tabla Comparativa con Speedup**

> **Speedup = tSecuencial / tParalelo**

###  **Resultados completos**


| n  | M√≥dulo          | Secuencial (ms) | Paralelo (ms) | Speedup |
|----|------------------|------------------|----------------|---------|
| **6** | Tiempos        | 0.209           | 1.0032         | 0.20√ó   |
|   -  | Programaciones  | 1.5434          | 5.1659         | 0.30√ó   |
|   -  | Costos          | 0.2143          | 1.7886         | 0.12√ó   |
|   -  | √ìptimo          | 3.6129          | 9.2645         | 0.39√ó   |
| **7** | Tiempos        | 0.0032          | 0.0769         | 0.04√ó   |
|   - | Programaciones   | 3.5706          | 4.0445         | 0.88√ó   |
|   - | Costos           | 0.0076          | 0.3835         | 0.02√ó   |
|   - | √ìptimo           | 18.7852         | 77.8895        | 0.24√ó   |
| **8** | Tiempos        | 0.0042          | 0.0719         | 0.06√ó   |
|   - | **Programaciones**   | **29.1254**     | **21.5941**    | **1.34√ó**|
|   - | Costos           | 0.0068          | 0.1912         | 0.03√ó   |
|   - | **√ìptimo**           | **183.5716**        | **174.384**        | **1.05√ó**|


----------

# 4. **Interpretaci√≥n de Resultados**

## 4.1 M√≥dulos lineales (O(n))

-   `tIR`, `costos`, `movilidad`  
    Tienen sobrecosto por:
    
-   creaci√≥n del scheduler
    
-   bifurcaci√≥n de threads
    
-   sincronizaci√≥n impl√≠cita
    

 **Conclusi√≥n:** Para operaciones peque√±as, el paralelismo **no vale la pena**.

----------

## 4.2 Generaci√≥n de permutaciones

El comportamiento mejora a partir de **n = 8**, donde el paralelismo obtiene:

-   **Speedup = 1.34√ó**
    

 Esto es consistente con la naturaleza factorial del problema.  
A mayor cantidad de trabajo, m√°s se amortiza el overhead.

----------

## 4.3 C√°lculo de programaci√≥n √≥ptima

Este m√≥dulo depende directamente de:

-   Generaci√≥n de permutaciones
    
-   C√°lculo de costos
    

Cuando `n=8`, los tiempos ya son suficientemente grandes para que el paralelismo sea rentable:

-   Speedup **1.05√ó**
    

Aunque peque√±o, indica que para entradas mayores, la versi√≥n paralela mejorar√° cada vez m√°s.

----------

# 5. Conclusiones Generales

‚úî La paralelizaci√≥n se aplic√≥ correctamente a las partes independientes del algoritmo  
‚úî Los m√≥dulos **lineales** no se benefician debido al bajo costo computacional  
‚úî Los m√≥dulos **factoriales** s√≠ muestran mejora al aumentar n  
‚úî El comportamiento obtenido coincide con los fundamentos te√≥ricos del paralelismo  
‚úî El dise√±o del paralelismo respeta el modelo ‚Äúdivide and conquer‚Äù y ‚Äúparallel-map‚Äù

###  Conclusi√≥n principal

> La paralelizaci√≥n en este proyecto es **efectiva √∫nicamente para los componentes con complejidad factorial**, donde la cantidad de trabajo supera ampliamente el costo de administraci√≥n del paralelismo.
    
---------